{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import similarity_metric as sm\n",
    "import formality_metric as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_from_string_to_integer(mungchi_string):\n",
    "    # 슬래시와 공백을 제외한 글자 수를 계산\n",
    "    parts = mungchi_string.split('/')  # 슬래시를 기준으로 문자열을 나눔\n",
    "    mungchi_integer = [len(part.strip()) for part in parts]  # 각 부분을 공백 제거 후 길이 계산\n",
    "    return mungchi_integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'lyrics', 'genre', 'check_only_korean', 'line_samples',\n",
      "       'verse_samples', 'total_samples', 'line_sample_word_mungchi_string',\n",
      "       'line_sample_word_mungchi_integer', 'line_sample_line_mungchi_string',\n",
      "       'line_sample_line_mungchi_integer', 'verse_sample_word_mungchi_string',\n",
      "       'verse_sample_word_mungchi_integer', 'verse_sample_line_mungchi_string',\n",
      "       'verse_sample_line_mungchi_integer', 'total_sample_word_mungchi_string',\n",
      "       'total_sample_word_mungchi_integer', 'total_sample_line_mungchi_string',\n",
      "       'total_sample_line_mungchi_integer'],\n",
      "      dtype='object')\n",
      "(6090, 19)\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_json('./../data/dataset_v2.0.json')\n",
    "print(dataframe.columns)\n",
    "print(dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframe.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inference\n",
    "- 가상의 테스트셋에 해당하는 노래 10곡\n",
    "- 여기서 input으로 요청하는 음절수가 golden_mungchi_integer에 저장되어야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 19)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_test_df = df.loc[df[\"check_only_korean\"]==True].sample(n=10, random_state=42)\n",
    "temp_test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# 의미 유사도 산출을 위한 encoder 불러오기\n",
    "# model = AutoModel.from_pretrained(\"kakaobank/kf-deberta-base\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"kakaobank/kf-deberta-base\")\n",
    "model = AutoModel.from_pretrained(\"klue/roberta-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "predict_mungchi_integer_list = []\n",
    "semantic_sim_list = []\n",
    "lexical_sim_list = []\n",
    "acc_form_list = []\n",
    "mse_form_list = []\n",
    "our_form_list = []\n",
    "\n",
    "for row in range(len(temp_test_df)):\n",
    "    \n",
    "    golden_lyrics = temp_test_df.iloc[row]['lyrics']\n",
    "    \n",
    "    # golden_mungchi_integer = 실제 input으로 들어가는 음절 수\n",
    "    # type : list [2,2,2]\n",
    "    golden_mungchi_integer = temp_test_df.iloc[row]['line_sample_word_mungchi_integer'][0]\n",
    "    # golden_mungchi_string = 생성된 가사와 비교해볼 string -> 실제로 사용은 안됨.\n",
    "    # type : 뭉치가 ' / '로 구분된 하나의 str ('내가 / 만든 / 가사')\n",
    "    golden_mungchi_string = temp_test_df.iloc[row]['line_sample_word_mungchi_string'][0]\n",
    "    # predict_mungchi_string = 동일 주제, 동일 장르로 생성된 string\n",
    "    # type : 뭉치가 ' / '로 구분된 하나의 str ('내가 / 만든 / 가사')\n",
    "    predict_mungchi_string = temp_test_df.iloc[row]['line_sample_word_mungchi_string'][0]\n",
    "    \n",
    "    predict_mungchi_integer = switch_from_string_to_integer(predict_mungchi_string)\n",
    "    predict_mungchi_integer_list.append(predict_mungchi_integer)\n",
    "    \n",
    "    # check log\n",
    "    # print(f'<golden_lyrics>\\n{golden_lyrics}\\n')\n",
    "    # print(f'<predict_mungchi>\\n{predict_mungchi_string}')\n",
    "    \n",
    "    # evaluate test data\n",
    "    semantic_sim = sm.eval_semantic_sim(model, tokenizer, golden_lyrics, predict_mungchi_string)\n",
    "    lexical_sim = sm.eval_lexical_sim_bleu(golden_lyrics, predict_mungchi_string)\n",
    "    acc_form, mse_form = fm.eval_form(golden_mungchi_integer, predict_mungchi_integer)\n",
    "    our_form = fm.eval_our_form(golden_mungchi_integer, predict_mungchi_integer)\n",
    "    \n",
    "    # save scores\n",
    "    semantic_sim_list.append(semantic_sim)\n",
    "    lexical_sim_list.append(lexical_sim)\n",
    "    acc_form_list.append(acc_form)\n",
    "    mse_form_list.append(mse_form)\n",
    "    our_form_list.append(our_form)\n",
    "\n",
    "eval_df = pd.DataFrame({'original_lyrics' : temp_test_df['lyrics'],\n",
    "                        'input_mungchi_integer' : temp_test_df['line_sample_word_mungchi_integer'],\n",
    "                        'input_mungchi_string' : temp_test_df['line_sample_word_mungchi_string'],\n",
    "                        'generated_mungchi_string' : temp_test_df['line_sample_word_mungchi_string'],\n",
    "                        'generated_mungchi_integer' : predict_mungchi_integer_list,\n",
    "                        'semantic_sim' : semantic_sim_list,\n",
    "                        'bleu_lexical_sim' : lexical_sim_list,\n",
    "                        'acc_form' : acc_form_list,\n",
    "                        'mse_form' : mse_form_list,\n",
    "                        'our_form' : our_form_list})\n",
    "\n",
    "# save evaluation result\n",
    "eval_df.to_csv('evalutation_result.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
